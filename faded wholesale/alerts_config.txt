# ========================================
# FADED SKIES API ALERTING RULES
# ========================================

groups:
  - name: faded-skies-api-alerts
    rules:
      # API Health Check
      - alert: APIDown
        expr: up{job="faded-skies-api"} == 0
        for: 1m
        labels:
          severity: critical
          service: faded-skies-api
        annotations:
          summary: "Faded Skies API is down"
          description: "The Faded Skies API has been down for more than 1 minute"
          runbook_url: "https://docs.fadedskies.com/runbooks/api-down"

      # High Response Time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, http_request_duration_ms_bucket{job="faded-skies-api"}) > 5000
        for: 5m
        labels:
          severity: warning
          service: faded-skies-api
        annotations:
          summary: "High API response time"
          description: "95th percentile response time is above 5 seconds for 5 minutes"

      # High Error Rate
      - alert: HighErrorRate
        expr: (rate(http_requests_total{job="faded-skies-api",status=~"5.."}[5m]) / rate(http_requests_total{job="faded-skies-api"}[5m])) > 0.05
        for: 5m
        labels:
          severity: warning
          service: faded-skies-api
        annotations:
          summary: "High API error rate"
          description: "Error rate is above 5% for 5 minutes"

      # Authentication Failures
      - alert: HighAuthFailureRate
        expr: rate(auth_failures_total{job="faded-skies-api"}[5m]) > 10
        for: 2m
        labels:
          severity: warning
          service: faded-skies-api
        annotations:
          summary: "High authentication failure rate"
          description: "Authentication failures are above 10 per minute"

      # Database Connection Issues
      - alert: DatabaseConnectionFailed
        expr: database_connections_failed_total{job="faded-skies-api"} > 0
        for: 1m
        labels:
          severity: critical
          service: faded-skies-api
        annotations:
          summary: "Database connection failures"
          description: "Database connection failures detected"

      # Memory Usage
      - alert: HighMemoryUsage
        expr: (process_resident_memory_bytes{job="faded-skies-api"} / 1024 / 1024) > 512
        for: 10m
        labels:
          severity: warning
          service: faded-skies-api
        annotations:
          summary: "High memory usage"
          description: "API memory usage is above 512MB for 10 minutes"

      # CPU Usage
      - alert: HighCPUUsage
        expr: rate(process_cpu_seconds_total{job="faded-skies-api"}[5m]) > 0.8
        for: 10m
        labels:
          severity: warning
          service: faded-skies-api
        annotations:
          summary: "High CPU usage"
          description: "API CPU usage is above 80% for 10 minutes"

  - name: faded-skies-business-alerts
    rules:
      # Low Product Stock
      - alert: LowProductStock
        expr: product_stock_count{job="faded-skies-api"} < 5
        for: 1m
        labels:
          severity: warning
          service: faded-skies-api
          type: business
        annotations:
          summary: "Low product stock alert"
          description: "Product {{ $labels.product_name }} has low stock: {{ $value }} units remaining"

      # High Order Volume
      - alert: HighOrderVolume
        expr: rate(orders_created_total{job="faded-skies-api"}[1h]) > 50
        for: 10m
        labels:
          severity: info
          service: faded-skies-api
          type: business
        annotations:
          summary: "High order volume"
          description: "Order creation rate is unusually high: {{ $value }} orders per hour"

      # Failed Payment Processing
      - alert: PaymentProcessingFailed
        expr: rate(payment_failures_total{job="faded-skies-api"}[5m]) > 1
        for: 2m
        labels:
          severity: critical
          service: faded-skies-api
          type: business
        annotations:
          summary: "Payment processing failures"
          description: "Payment processing failures detected: {{ $value }} failures in last 5 minutes"

      # New Partner Registrations
      - alert: NewPartnerRegistration
        expr: increase(partner_registrations_total{job="faded-skies-api"}[1h]) > 0
        for: 1m
        labels:
          severity: info
          service: faded-skies-api
          type: business
        annotations:
          summary: "New partner registration"
          description: "{{ $value }} new partner registration(s) in the last hour"

  - name: faded-skies-infrastructure-alerts
    rules:
      # System Load
      - alert: HighSystemLoad
        expr: node_load1{job="node-exporter"} > 2.0
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High system load"
          description: "System load is high: {{ $value }}"

      # Disk Space
      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes{job="node-exporter",mountpoint="/"} / node_filesystem_size_bytes{job="node-exporter",mountpoint="/"}) < 0.1
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Low disk space"
          description: "Disk space is below 10%: {{ $value | humanizePercentage }}"

      # Memory Usage
      - alert: HighSystemMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"})) > 0.9
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High system memory usage"
          description: "System memory usage is above 90%"

      # Nginx Status
      - alert: NginxDown
        expr: up{job="nginx"} == 0
        for: 1m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Nginx is down"
          description: "Nginx reverse proxy is not responding"

      # PostgreSQL Status
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding"

      # Redis Status
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding"

      # SSL Certificate Expiry
      - alert: SSLCertificateExpirySoon
        expr: (ssl_certificate_expiry_seconds{job="faded-skies-api"} - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{ $labels.domain }} expires in {{ $value | humanizeDuration }}"

      # SSL Certificate Expired
      - alert: SSLCertificateExpired
        expr: ssl_certificate_expiry_seconds{job="faded-skies-api"} < time()
        for: 1m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "SSL certificate expired"
          description: "SSL certificate for {{ $labels.domain }} has expired"

  - name: faded-skies-security-alerts
    rules:
      # Suspicious Activity
      - alert: SuspiciousActivity
        expr: rate(suspicious_requests_total{job="faded-skies-api"}[5m]) > 5
        for: 2m
        labels:
          severity: warning
          service: security
        annotations:
          summary: "Suspicious activity detected"
          description: "High rate of suspicious requests: {{ $value }} per minute"

      # Rate Limiting Triggered
      - alert: RateLimitingTriggered
        expr: rate(rate_limit_exceeded_total{job="faded-skies-api"}[5m]) > 10
        for: 5m
        labels:
          severity: info
          service: security
        annotations:
          summary: "Rate limiting frequently triggered"
          description: "Rate limiting is being triggered frequently: {{ $value }} times per minute"

      # Failed Login Attempts
      - alert: HighFailedLoginAttempts
        expr: rate(failed_login_attempts_total{job="faded-skies-api"}[5m]) > 20
        for: 5m
        labels:
          severity: warning
          service: security
        annotations:
          summary: "High failed login attempt rate"
          description: "High rate of failed login attempts: {{ $value }} per minute from IP {{ $labels.ip }}"

      # Unauthorized Access Attempts
      - alert: UnauthorizedAccessAttempts
        expr: rate(unauthorized_access_total{job="faded-skies-api"}[5m]) > 5
        for: 2m
        labels:
          severity: warning
          service: security
        annotations:
          summary: "Unauthorized access attempts"
          description: "Multiple unauthorized access attempts detected"

# ========================================
# ALERTMANAGER CONFIGURATION
# Create alertmanager.yml:
#
# global:
#   smtp_smarthost: 'localhost:587'
#   smtp_from: 'alerts@fadedskies.com'
#   slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
#
# route:
#   group_by: ['alertname', 'service']
#   group_wait: 10s
#   group_interval: 10s
#   repeat_interval: 1h
#   receiver: 'web.hook'
#   routes:
#     - match:
#         severity: critical
#       receiver: 'critical-alerts'
#     - match:
#         type: business
#       receiver: 'business-alerts'
#     - match:
#         service: security
#       receiver: 'security-alerts'
#
# receivers:
#   - name: 'web.hook'
#     slack_configs:
#       - channel: '#alerts'
#         title: 'Faded Skies Alert'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
#
#   - name: 'critical-alerts'
#     email_configs:
#       - to: 'admin@fadedskies.com'
#         subject: 'CRITICAL: {{ .GroupLabels.alertname }}'
#         body: |
#           {{ range .Alerts }}
#           Alert: {{ .Annotations.summary }}
#           Description: {{ .Annotations.description }}
#           {{ end }}
#     slack_configs:
#       - channel: '#critical'
#         title: 'CRITICAL ALERT'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
#         color: 'danger'
#
#   - name: 'business-alerts'
#     slack_configs:
#       - channel: '#business'
#         title: 'Business Alert'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
#         color: 'warning'
#
#   - name: 'security-alerts'
#     email_configs:
#       - to: 'security@fadedskies.com'
#         subject: 'SECURITY ALERT: {{ .GroupLabels.alertname }}'
#     slack_configs:
#       - channel: '#security'
#         title: 'SECURITY ALERT'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
#         color: 'danger'
# ========================================